{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dC7ars9FDERd",
        "ihO6cGG18MDx",
        "N64sszEQIrNY",
        "IPA98TnnYkkC",
        "hn08iYpmQ2PK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chabon6/LegalTeachingAssistant/blob/main/%E3%80%90Advanced_RAG%E3%80%91Legal_Teaching_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Text Function"
      ],
      "metadata": {
        "id": "dC7ars9FDERd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Paa-PjGy6Ngd",
        "outputId": "37cdd7ba-ec2b-475d-dfe2-fe3648a25b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (0.1.134)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (4.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_core) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_core) (3.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_core) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (2.23.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain_core) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain_core) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_core) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langchain.schema import Document\n",
        "\n",
        "class LawParser:\n",
        "    def __init__(self, text_content):\n",
        "        self.text_content = text_content\n",
        "        self.law_name = self.extract_law_name()\n",
        "\n",
        "    def extract_law_name(self):\n",
        "        \"\"\"\n",
        "        提取文本中的法規名稱，格式為「法規名稱：XXX」。\n",
        "\n",
        "        返回:\n",
        "        str: 匹配到的法規名稱，如果沒有匹配則返回空字串。\n",
        "        \"\"\"\n",
        "        pattern = r'法規名稱：(\\w+)'\n",
        "        match = re.search(pattern, self.text_content)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        else:\n",
        "            print(\"未找到法規名稱\")\n",
        "            return None\n",
        "\n",
        "    def extract_text_between_keywords(self, text, start_keyword, end_keyword, end=False):\n",
        "        \"\"\"\n",
        "        從指定文本中，抓取兩個關鍵詞之間的文字塊。\n",
        "\n",
        "        參數:\n",
        "        - text: 要搜索的文本。\n",
        "        - start_keyword: 開始關鍵詞。\n",
        "        - end_keyword: 結束關鍵詞。\n",
        "        - end: True or False 沒有end_keyword時填 True。\n",
        "\n",
        "        返回:\n",
        "        - 提取出的文字塊，若未找到匹配，返回 None。\n",
        "        \"\"\"\n",
        "        if not end:\n",
        "            pattern = rf\"{re.escape(start_keyword)}(.+?){re.escape(end_keyword)}\"\n",
        "        else:\n",
        "            pattern = rf\"{re.escape(start_keyword)}(.+)$\"\n",
        "        match = re.search(pattern, text, re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        else:\n",
        "            print('未找到文字塊')\n",
        "            return None\n",
        "\n",
        "    def extract_provisions_names(self, text, provisions):\n",
        "        \"\"\"\n",
        "        提取文本中的章節名稱，格式為「第 X 章(or節or目)...」的部分。\n",
        "\n",
        "        參數:\n",
        "        text (str): 要搜索的文本。\n",
        "        provisions (str): 決定章or節or目\n",
        "\n",
        "        返回:\n",
        "        list: 匹配到的章、節、目名稱列表，若無匹配則返回空列表。\n",
        "        \"\"\"\n",
        "        if provisions == '條':\n",
        "            pattern = r'(第 \\d+-?\\w? 條)'\n",
        "        else:\n",
        "            pattern = rf'(第 \\w+ {re.escape(provisions)}.+)'\n",
        "        match = re.findall(pattern, text)\n",
        "        if match:\n",
        "            return match\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def provisions_split_list(self, text, provision):\n",
        "        \"\"\"\n",
        "        根據章、節、目名稱，將文本分割成列表。\n",
        "\n",
        "        參數:\n",
        "        text (str): 要分割的文本。\n",
        "        provision (str): 決定章or節or目\n",
        "\n",
        "        返回:\n",
        "        list: 分割後的章、節、目名稱列表。\n",
        "        \"\"\"\n",
        "        splited_provisions_list = []\n",
        "        names = self.extract_provisions_names(text, provision)\n",
        "        provision_index = {'章': 1, '節': 2, '目': 3, '條': 4}.get(provision, 0)\n",
        "\n",
        "        if names:\n",
        "            pattern = r'\\[.+\\]'\n",
        "            match = re.findall(pattern, text)\n",
        "\n",
        "            for i, name in enumerate(names):\n",
        "                if i != len(names) - 1:\n",
        "                    extract_text = self.extract_text_between_keywords(text, name, names[i + 1])\n",
        "                else:\n",
        "                    extract_text = self.extract_text_between_keywords(text, name, \"\", end=True)\n",
        "                splited_provisions_list.append(f\"{match[0] if match else ''} [{provision_index}_{name}] \" + extract_text)\n",
        "        else:\n",
        "            splited_provisions_list.append(text)\n",
        "        return splited_provisions_list\n",
        "\n",
        "    def law_split_list(self):\n",
        "        \"\"\"\n",
        "        切出條為單位的chunks。\n",
        "\n",
        "        返回:\n",
        "        list: 切分後的章、節、目、條的列表。\n",
        "        \"\"\"\n",
        "        Chapter_list = self.provisions_split_list(self.text_content, \"章\")\n",
        "        Section_list = []\n",
        "        for splited_text in Chapter_list:\n",
        "            Section_list += self.provisions_split_list(splited_text, \"節\")\n",
        "\n",
        "        Item_list = []\n",
        "        for splited_text in Section_list:\n",
        "            Item_list += self.provisions_split_list(splited_text, \"目\")\n",
        "\n",
        "        final_list = []\n",
        "        for splited_text in Item_list:\n",
        "            final_list += self.provisions_split_list(splited_text, \"條\")\n",
        "        return final_list\n",
        "\n",
        "    def list2doc(self, final_list):\n",
        "        \"\"\"\n",
        "        每一條文轉為 Langchain 的 Document。\n",
        "\n",
        "        返回:\n",
        "        list: 轉換後的 Document 列表。\n",
        "        \"\"\"\n",
        "        for i, text in enumerate(final_list):\n",
        "            source = re.findall(r'\\[.+\\]', text)[0]\n",
        "            try:\n",
        "                Chapter = re.findall(r'\\[1.+?\\]', text)[0]\n",
        "            except:\n",
        "                Chapter = ''\n",
        "            try:\n",
        "                Section = re.findall(r'\\[2.+?\\]', text)[0]\n",
        "            except:\n",
        "                Section = ''\n",
        "            try:\n",
        "                Item = re.findall(r'\\[3.+?\\]', text)[0]\n",
        "            except:\n",
        "                Item = ''\n",
        "            try:\n",
        "                Article = re.findall(r'\\[4.+?\\]', text)[0]\n",
        "            except:\n",
        "                Article = ''\n",
        "            final_list[i] = Document(page_content=text, metadata={\"law_name\": self.law_name, \"Chapter\": Chapter, \"Section\": Section, \"Item\": Item, \"Article\": Article})\n",
        "        return final_list"
      ],
      "metadata": {
        "id": "NwgMwr-y5eD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Text"
      ],
      "metadata": {
        "id": "ihO6cGG18MDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 連結雲端\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/LangChain練習/RAG_LegalTeachingAssistant/Law_rtf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIy8dQvrDry3",
        "outputId": "48461621-9330-4b34-f59b-77595e80a60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 在file_path底下遍歷rtf檔\n",
        "from striprtf.striprtf import rtf_to_text\n",
        "import os\n",
        "\n",
        "\n",
        "final_docs = []\n",
        "for i, filename in enumerate(os.listdir(file_path)):\n",
        "    if filename.endswith('.rtf'):\n",
        "        print(f\"Processing file: {filename}\")\n",
        "        # 讀取 RTF 文件\n",
        "        with open(os.path.join(file_path, filename), 'r', encoding='utf-8') as file:\n",
        "            rtf_content = file.read()\n",
        "        # 轉換為純文本\n",
        "        text_content = rtf_to_text(rtf_content)\n",
        "\n",
        "        # 切割法典並轉成document\n",
        "        parser = LawParser(text_content)\n",
        "        final_list = parser.law_split_list()\n",
        "        final_docs.append(parser.list2doc(final_list))\n",
        "        print(f\"Example: {final_docs[i][-1]}\")\n",
        "        print(\"Finished...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va7FveUj06hX",
        "outputId": "8451432b-6adb-4930-c2cf-d0ab572cee90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: 公司法.rtf\n",
            "Example: page_content='[1_第 九 章 附則] [4_第 449 條] 本法除中華民國八十六年六月二十五日修正公布之第三百七十三條及第三百八十三條、一百零四年七月一日修正公布之第五章第十三節條文、一百零七年七月六日修正之條文之施行日期由行政院定之，及九十八年五月二十七日修正公布之條文自九十八年十一月二十三日施行外，自公布日施行。' metadata={'law_name': '公司法', 'Chapter': '[1_第 九 章 附則]', 'Section': '', 'Item': '', 'Article': '[4_第 449 條]'}\n",
            "Finished...\n",
            "Processing file: 商業會計法.rtf\n",
            "Example: page_content='[1_第 十 章 附則] [4_第 83 條] 1   本法自公布日施行。\n",
            "2   本法中華民國一百零三年五月三十日修正之條文，自一百零五年一月一日施行。但商業得自願自一百零三年會計年度開始日起，適用中華民國一百零三年五月三十日修正之條文。' metadata={'law_name': '商業會計法', 'Chapter': '[1_第 十 章 附則]', 'Section': '', 'Item': '', 'Article': '[4_第 83 條]'}\n",
            "Finished...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RetrievalQA"
      ],
      "metadata": {
        "id": "N64sszEQIrNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Package"
      ],
      "metadata": {
        "id": "IPA98TnnYkkC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLmQD0rn669R"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-openai langchain_community langchain_experimental\n",
        "!pip install --upgrade --quiet langchain-chroma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get API\n",
        "from google.colab import userdata\n",
        "# 文本 Embedding\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "# 向量儲存庫\n",
        "from langchain_chroma import Chroma\n",
        "# RAG\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "# 用於解析 LLM 輸出的基底\n",
        "from langchain_core.output_parsers import BaseOutputParser\n",
        "# 用於定義 prompt 模板\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "# ger api\n",
        "gpt_api = userdata.get('gpt_api')\n",
        "\n",
        "from  langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "ZICeUy4poCqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding"
      ],
      "metadata": {
        "id": "3sMBU8L5YhSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "persist_directory = '/content/drive/MyDrive/LangChain練習/RAG_LegalTeachingAssistant/Chroma_db'\n",
        "# 有檔案就讀檔案\n",
        "if len(os.listdir(persist_directory)) > 1:\n",
        "    # 初始化 OpenAI 的 Embedding 模型\n",
        "    gpt_api = userdata.get('gpt_api')\n",
        "    embedding_model = OpenAIEmbeddings(openai_api_key=gpt_api)\n",
        "    # 載入儲存在本地的 Chroma 資料庫\n",
        "    chroma_db = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
        "    print(\"已載入Chroma資料庫\")\n",
        "else:\n",
        "    # 初始化 OpenAI 的 Embedding 模型\n",
        "    embedding_model = OpenAIEmbeddings(openai_api_key=gpt_api)\n",
        "\n",
        "    # Embedding 並儲存至向量儲存庫（另存一份到本地）\n",
        "    chroma_db = Chroma.from_documents(final_docs, embedding_model, persist_directory=persist_directory)\n",
        "    print(\"已儲存Chroma資料庫\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eft2ktaGZKX_",
        "outputId": "d6a1faae-6903-487d-d1f4-b0a9e1f25f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "已儲存Chroma資料庫\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt"
      ],
      "metadata": {
        "id": "quGv7TpYmioS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "# template\n",
        "prompt_template = PromptTemplate(\n",
        "                  input_variables = [\"Quation\", \"Chunks\"],\n",
        "                  template = '''\n",
        "                  <rule>你是一位專業的中華民國法律顧問，負責回答使用者的法律諮詢</rule>\n",
        "                  <task>根據檢索的文本(chunks)回答問題，並且另外附上檢索來源，例如XX法XX章第X條。若你不知道答案就回答\"不知道\"</task>\n",
        "                  <Quation>{Quation}</Quation>\n",
        "                  <chunks>{Chunks}</chunks>\n",
        "                  <format>\n",
        "                  結論：\n",
        "                  原因：\n",
        "                  各選項所對應的完整法條：\n",
        "                    (A)\n",
        "                    (B)\n",
        "                    (C)\n",
        "                    (D)\n",
        "                  </format>'''\n",
        "                  )\n",
        "# 設定使用者提問\n",
        "\n",
        "Quation = '''\n",
        "6 甲於大學畢業後擬與同學 8 人共同創業，並打算設立 A 有限公司（下稱「A 公司」），下列關於 A 公司之敘述，何者錯誤？\n",
        "(A) A 公司最少置董事 3 人\n",
        "(B) 董事應經股東表決權三分之二以上之同意，就有行為能力之股東中選任之\n",
        "(C) 除非有特約，否則董事不得向公司請求報酬\n",
        "(D) 若甲被選為董事，不得無故辭任，亦不得無故使其退職\n",
        "'''"
      ],
      "metadata": {
        "id": "QmvzeN-RQIn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query Transformation(abandon)"
      ],
      "metadata": {
        "id": "hn08iYpmQ2PK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 將 LLM 的換行輸出轉換成 list\n",
        "# from typing import List\n",
        "# class LineListOutputParser(BaseOutputParser[List[str]]):\n",
        "#     \"\"\"Output parser for a list of lines.\"\"\"\n",
        "#     def parse(self, text: str) -> List[str]:\n",
        "#         lines = text.strip().split(\"\\n\")\n",
        "#         return list(filter(None, lines))  # Remove empty lines\n",
        "\n",
        "# output_parser = LineListOutputParser()\n",
        "\n",
        "# # 用於生成多個相似問題的 Prompt\n",
        "# QUERY_PROMPT = PromptTemplate(\n",
        "#     input_variables=[\"question\"],\n",
        "#     template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "#     different versions of the given user question to retrieve relevant documents from a vector\n",
        "#     database. By generating multiple perspectives on the user question, your goal is to help\n",
        "#     the user overcome some of the limitations of the distance-based similarity search.\n",
        "#     Provide these alternative questions separated by newlines.\n",
        "#     Original question: {question}\"\"\",\n",
        "# )\n",
        "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=gpt_api)\n",
        "\n",
        "# # MultiQueryRetriever 的 Chain\n",
        "# llm_chain = QUERY_PROMPT | llm | output_parser\n",
        "\n",
        "# # 設定檢索器\n",
        "# retriever = MultiQueryRetriever(\n",
        "#     retriever=chroma_db.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
        "# )  # \"lines\" is the key (attribute name) of the parsed output\n",
        "\n",
        "# # 檢索結果\n",
        "# unique_docs = retriever.invoke(query)\n",
        "# len(unique_docs)\n",
        "\n",
        "# prompt: 接著將RAG得到的unique_docs與原始提問query一同交給LLM回答\n",
        "\n",
        "# qa_chain = RetrievalQA.from_chain_type(\n",
        "#     llm=llm,\n",
        "#     chain_type=\"stuff\",\n",
        "#     retriever=retriever,\n",
        "#     return_source_documents=True\n",
        "# )\n",
        "\n",
        "# result = qa_chain({\"query\": query})\n",
        "\n",
        "# print(result[\"result\"])\n",
        "# print(\"Source Documents:\")\n",
        "# for document in result[\"source_documents\"]:\n",
        "#     print(document.page_content)\n",
        "#     print(\"---\")\n"
      ],
      "metadata": {
        "id": "wK5hiiV3Wr2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Small-to-Big"
      ],
      "metadata": {
        "id": "65RVzmthefj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化檢索器\n",
        "retriever = chroma_db.as_retriever()\n",
        "# 設定檢索器返回多個段落\n",
        "retriever.search_kwargs = {\"k\": 3}  # 設定檢索結果數量為 3\n",
        "# 以使用者提問 Quation 為檢索參考而非 Prompt\n",
        "unique_docs = retriever.invoke(Quation)"
      ],
      "metadata": {
        "id": "XqnnQFGrU7s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 找出上一層的章or節or目\n",
        "def get_targets(unique_docs):\n",
        "    targets = {}\n",
        "    for doc in unique_docs:\n",
        "        for metadata_key in ['Item', 'Section', 'Chapter']:\n",
        "            if doc.metadata[metadata_key] != '':\n",
        "                try:\n",
        "                  targets[metadata_key].add(doc.metadata[metadata_key])\n",
        "                except:\n",
        "                  targets[metadata_key] = set()\n",
        "                  targets[metadata_key].add(doc.metadata[metadata_key])\n",
        "                break\n",
        "    return targets"
      ],
      "metadata": {
        "id": "EU7-Vzk6QwxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets = get_targets(unique_docs)\n",
        "# 把上一層的所有條文全部索引出來\n",
        "total_chunks = ''\n",
        "for target in targets.keys():\n",
        "    for i in targets[target]:\n",
        "        total_chunks += str(chroma_db.get(where={target: i})['documents'])"
      ],
      "metadata": {
        "id": "NmESozfzdueA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化 OpenAI 的 LLM 模型\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=gpt_api)\n",
        "\n",
        "# 提示詞\n",
        "prompt = prompt_template.format(Quation=Quation, Chunks=total_chunks)\n",
        "\n",
        "#\n",
        "result = llm.invoke(prompt)\n"
      ],
      "metadata": {
        "id": "OMql7Z9kpY5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1erSNw6zpL_f",
        "outputId": "1b4cbc0c-87a0-4a42-a074-5e586bdfb724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "結論：  \n",
            "選項 (B) 錯誤。\n",
            "\n",
            "原因：  \n",
            "根據《公司法》第192條，董事應由股東會就有行為能力之人選任之，並未規定需經股東表決權三分之二以上之同意。\n",
            "\n",
            "各選項所對應的完整法條：  \n",
            "(A) 《公司法》第192條第1項：公司董事會，設置董事不得少於三人，由股東會就有行為能力之人選任之。  \n",
            "(B) 《公司法》第192條第1項：公司董事會，設置董事不得少於三人，由股東會就有行為能力之人選任之。  \n",
            "(C) 《公司法》第196條第1項：董事之報酬，未經章程訂明者，應由股東會議定，不得事後追認。  \n",
            "(D) 《公司法》第199條第1項：董事得由股東會之決議，隨時解任；如於任期中無正當理由將其解任時，董事得向公司請求賠償因此所受之損害。\n"
          ]
        }
      ]
    }
  ]
}